

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }
    td {
    line-height: 130%;
    }
    tr.spaceUnder>td {
        padding-bottom: 10px;
    }
    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>Long-term Human Motion Prediction with Scene Context</title>
        <meta property="og:title" content="sceneflow" />
        <meta property="og:url" content="https://www.youtube.com/watch?v=cYHQKtBLI3Q" />
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">Long-term Human Motion Prediction with Scene Context </span>
    <br>
    <br>
      <table align=center width=800px>


     <tr>
       <span style="font-size:22px"><a href="https://zhec.github.io/">Zhe Cao</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~hangg/">Hang Gao</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://karttikeya.github.io/">Karttikeya Mangalam</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://scholar.google.com/citations?user=oyh-YNwAAAAJ&hl=en">Qi-Zhi Cai</a></span><sup>2</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://minhpvo.github.io/">Minh Vo</a></span><sup>3</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik</a></span><sup>1</sup>
   </tr>

     <tr>
       <td align=center colspan="2" style="font-size:22px">
       <center>
       	<sup>1</sup> UC Berkeley &nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup> Nanjing University&nbsp;&nbsp;&nbsp;&nbsp;<sup>3</sup> Facebook Reality Labs
       </center>
       </td>

      </tr>
    </table>

          <br>
          <br>
          <center>
            <table align=center width=700>
                <tr>
                  <td><video width="700px" controls> <source src="eccv-long.mp4" type=video/mp4><video></td>
              </tr>
            </table>
          </center>
          <br>

          <hr>
          <center><h1>Abstract</h1></center>
          <table align=center width=600px>
              <tr>
                  <td width=700px>
                    <center>
                        <img src = "teaser.png" height="180px"></img><br>
                  </center>
                  </td>
              </tr>

              <tr>
                <td colspan="3"> <br>
                  Human movement is goal-directed and influenced by the spatial layout of the objects in the scene. To plan future human motion, it is crucial to perceive the environment -- imagine how hard it is to navigate a new room with lights off.
                  Existing works on predicting human motion do not pay attention to the scene context and thus struggle in long-term prediction.
                  In this work, we propose a novel three-stage framework that exploits scene context to tackle this task, as shown in the above image.
                  Given a single scene image and 2D pose histories, our method first samples multiple human motion goals, then plans 3D human paths towards each goal, and finally predicts 3D human pose sequences following each path.
                  For stable training and rigorous evaluation, we contribute a diverse synthetic dataset with clean annotations.
                  In both synthetic and real datasets, our method shows consistent quantitative and qualitative improvements over existing methods.
		</td>
              </tr>
          </table>


        <hr>
         <!-- <table align=center width=550px> -->
                <center><h1>Results</h1></center>
                <table align=center width=900px>
                    <tr>
                        <td width=1200px colspan="3">
                          <center>
                              <img src = "qualitative.jpg" height="1080px"></img><br>
                        </center>
                        </td>
                    </tr>
                    <tr>
                      <td align=center width=400px>
                        (a)
                      </td>
                      <td align=center width=400px>
                        (b)
                      </td>
                      <td align=center width=400px>
                        (c)
                      </td>
                  </tr>

                    <tr>
                        <td width=600px colspan="3">
                              <span style="font-size:14px"><i>Qualitative results of our method. (a) input: short-term 2D human pose sequence and a single RGB image, (b-c): future human motion predictions with consideration of the scene context. We visualize 3D human poses in ground-truth point cloud and change the color gradually from purple to dark blue, and eventually light blue across time steps. The top two rows show our three-second-long prediction results in GTA-IM dataset and the bottom two rows show our two-second-long prediction results in PROX dataset. To best visualize the 3D poses, we may rotate the camera viewpoint slightly for visualizing the two predictions from the same input sequence. Our method can generate diverse human motion, e.g., turning left/right, walking straight, taking a u-turn, climbing stairs, standing up from sitting, and laying back on the sofa. </i>
                        </td>
                    </tr>
                </table>
                <br>
      <hr>
      <table align=center width=800>
       <center><h1>GTA-IM Dataset</h1></center>
          <tr class="spaceUnder">
            <td>
              <img style="width:280px" src="demo1.gif"/>
            </td>
            <td>
              <img style="width:280px" src="demo2.gif"/>
            </td>
            <td>
              <img style="width:280px" src="demo3.gif"/>
            </td>
        </tr>

        <tr>
          <td>
            <img style="width:280px" src="demo4.gif"/>
          </td>
          <td>
            <img style="width:280px" src="demo5.gif"/>
          </td>
          <td>
            <img style="width:280px" src="demo6.gif"/>
          </td>
      </tr>

        <tr>
          <td align=center><br>
            <span style="font-size:24px">&nbsp;<a href='https://zhec.github.io/hmp/sample_1.zip'>[Sample #1]</a>
	  </td>
          <td align=center><br>
            <span style="font-size:24px">&nbsp;<a href='https://zhec.github.io/hmp/sample_2.zip'>[Sample #2]</a>
	  </td>
          <td align=center><br>
            <span style="font-size:24px">&nbsp;<a href='#'>[Sample #3]</a>
          </td>
        <tr>
        <tr>
          <td colspan="3"> <br>
          We introduce the GTA Indoor Motion dataset (GTA-IM) that emphasizes human-scene interactions in the indoor environments. We collect HD RGB-D image seuqences of 3D human motion from realistic game engine. The dataset has clean 3D human pose and camera pose annoations, and large diversity in human appearances, camera views, and activities: 10 different large house models, 13 weathers, 50 human models, 22 walking styles, and various actions. 
	  </td>
        </tr>
        <tr>
          <td colspan="3"> <br>
            To download the dataset, please follow the instruction in our <a href="https://github.com/ZheC/GTA-IM-Dataset">GitHub repo</a>.
          </td>
        </tr>
      </table>
    <br>

    <hr>
      <table align=center width=800>
       <center><h1>Paper</h1></center>
          <tr>
            <td><a href="preprint.pdf"><img style="width:400px" src="thumbnail.png"/></a></td>
            <td><span style="font-size:14pt">Cao, Gao, Mangalam, Cai, Vo, Malik.<br><br>
              Long-term Human Motion Prediction<br> with Scene Context<br><br>
             ECCV 2020 (Oral). <br><br>
		<a href="https://arxiv.org/pdf/2007.03672.pdf">[Paper]</a> &nbsp; &nbsp;
                <a href="bibtex.txt">[Bibtex]</a>
              </td>
        </tr>
      </table>
    <br>

      <hr>
            <table align=center width=1100px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                We thank members of the BAIR community for helpful discussions and comments. This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>

